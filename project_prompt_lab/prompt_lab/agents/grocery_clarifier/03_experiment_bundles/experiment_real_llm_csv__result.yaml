id: experiment__real_llm_csv
step: step_01_scoring
model:
  type: llm
  provider: openai
  name: gpt-4.1-mini
prompt:
  purpose: Real LLM batch clarity scoring (CSV input)
  text: |
    You are a shopping assistant.
    Evaluate the following shopping notes and return a clarity score (0â€“100) for each line.
    Notes:
    {csv}
input:
  format: csv
  content:
    file: experiment_real_llm_csv_input.csv
initial_message: ""
expected_output:
  - clarity_score: 20
    interpreted_text: null
  - clarity_score: 60
    interpreted_text: null
  - clarity_score: 10
    interpreted_text: null

actual_output:
  display_message: "1. tej - Clarity score: 40  \n2. I have to buy 200g 82% fat content\
    \ butter - Clarity score: 95  \n3. asdfasfdf - Clarity score: 0"
  parsed_response:
    tool_used: false
    tool_name: null
    tool_args: null
    message: "1. tej - Clarity score: 40  \n2. I have to buy 200g 82% fat content\
      \ butter - Clarity score: 95  \n3. asdfasfdf - Clarity score: 0"
  raw_response: !!python/object:langchain_core.messages.ai.AIMessage
    __dict__:
      content: "1. tej - Clarity score: 40  \n2. I have to buy 200g 82% fat content\
        \ butter - Clarity score: 95  \n3. asdfasfdf - Clarity score: 0"
      additional_kwargs:
        refusal: null
      response_metadata:
        token_usage:
          completion_tokens: 48
          prompt_tokens: 252
          total_tokens: 300
          completion_tokens_details:
            accepted_prediction_tokens: 0
            audio_tokens: 0
            reasoning_tokens: 0
            rejected_prediction_tokens: 0
          prompt_tokens_details:
            audio_tokens: 0
            cached_tokens: 0
        model_name: gpt-4.1-mini-2025-04-14
        system_fingerprint: fp_6f2eabb9a5
        id: chatcmpl-BctNvrV8OVwfalWHv9aPVqgmSrHTJ
        service_tier: default
        finish_reason: stop
        logprobs: null
      type: ai
      name: null
      id: run--946028db-f7ed-49c7-b17e-32c95cce3d4b-0
      example: false
      tool_calls: []
      invalid_tool_calls: []
      usage_metadata:
        input_tokens: 252
        output_tokens: 48
        total_tokens: 300
        input_token_details:
          audio: 0
          cache_read: 0
        output_token_details:
          audio: 0
          reasoning: 0
    __pydantic_extra__: {}
    __pydantic_fields_set__: !!set
      additional_kwargs: null
      id: null
      usage_metadata: null
      invalid_tool_calls: null
      response_metadata: null
      content: null
      name: null
      tool_calls: null
    __pydantic_private__: null
validation:
  result:
    status: warning
    message: Expected a list, but got a different type.
    expected_type: list
    actual_type: dict
    full_match: false
log:
  status: warning
  path: 05_logs\experiment_real_llm_csv__log.md
meta:
  last_run: '2025-05-30T14:43:54'
  runner: runner
