# AI-Human Project Journal

- now 2025-05-17
- Started MVP 2 implementation: designed and implemented the pilot LLM agent (class, schema, test, and example usage) as the first step.
- Added a pilot micro-project (minimal LLM agent prototype) as a required first step in MVP 2 to validate structured LLM output before full integration.
- Added a detailed MVP 2 implementation plan (LLM batch processing, no clarification loop) to implementation_plan.md, including goal, checklist, AI suggestions, and progress tracking.
- Completed MVP 1 (core batch note interpreter): all tests pass, the pipeline works end-to-end, and documentation/examples are in place.
- Verified that all main documentation files (system overview, technical spec, functional spec, implementation plan) are consistent and aligned with the latest requirements (metadata fields, output format, batch agent, etc.).
- Decided to keep a running journal of project iterations and key decisions, with newest entries at the top, and to use a '- now [date]' marker for each day's records.
- Added detailed MVP milestones to the implementation plan for incremental delivery and validation.
- Established a clear, testable roadmap for the project with milestone-based MVPs.
- Restored and improved the technical specification, clarifying the batch-oriented LLM agent design and class structure.
- Renamed and reorganized documentation files for clarity and consistency.
- now 2025-05-18
- Refactored llm_agent.py to use more descriptive variable names for prompt injection, renamed template placeholders for clarity, and added a print statement to show the fully rendered prompt for debugging.
- Updated llm_agent.py to make the system prompt more explicit about always providing both 'entries' and 'new_memory_points' fields, and added a fallback in the tool function to handle missing fields gracefully.
- Clarified the required structure for 'entries' in the system prompt and tool docstring, and added validation to skip non-dict entries in the tool function in llm_agent.py.
- Implemented a clarification loop in llm_agent.py: the agent can ask clarification questions up to a maximum number of rounds, collects user answers, and finalizes with placeholders if the maximum is reached, following the functional specification.
- Updated the Classification YAML in the functional specification to include all entity_types and intents from the glossary for comprehensive metadata alignment.
- Set temperature=0.0 for all real LLM tests and the agent to ensure deterministic output, and updated both the agent and test code to support this.
- Changed DataEntry schema to match the system prompt and tool output (interpreted_text, entity_type, intent, clarity_score), updated all code and tests accordingly for consistency and correctness.
- Fixed an indentation bug in the clarification loop in llm_agent.py.
- Added a helper method to LLMAgent to detect fallback/placeholder output.
- Updated real LLM tests to assert that fallback output is not returned, with clear error messages.
- Ensured temperature=0.0 is always set for deterministic output in real LLM tests.
- Improved debug logging for tool outputs and fallback cases in the agent.
- Fixed a TypeError in llm_agent.py by updating the log.debug call to use a single string argument for section order logging.
- now 2024-06-09
- Implemented a config-driven, registry-based prompt builder (SystemPromptBuilder) that builds prompts from a YAML config file.
- Each prompt section is now a registry function, and the prompt structure/content is fully controlled by config.
- Added resources/prompt_config.yaml as the default config, supporting section order, enable/disable, and custom text/overrides.
- Refactored all prompt section logic to use the new registry system, enabling ultimate flexibility and extensibility.
- Added a params field to the classification section in prompt_config.yaml, allowing the classification YAML file to be specified in config.
- Updated SystemPromptBuilder to load the classification config from the file specified in the config, making the prompt builder fully config-driven for classification as well.
- Merged output_schema and output_field_meanings into a single output_schema_and_meanings section, loaded from YAML.
- Updated resources/schema.yaml to contain both type and description for each field.
- Updated SystemPromptBuilder and prompt_config.yaml to use the new unified section, making schema and field docs fully data-driven.
- Added stricter, correctness-based real LLM tests, improved output printing, and a guide for interactive clarification feedback in tests/test_llm_agent_real.py.
- Megbeszéltük a Python csomagolás, fejlesztői telepítés és projektstruktúra előnyeit, és létrehoztuk a setup.py-t a note_interpreter csomaghoz.
- Átbeszéltük a docs mappa szerepét, és hogy mi kerül a csomagba, mi nem.
- Elmagyaráztuk a sys.path trükk és a csomagolás közötti különbséget, és hogy mikor melyik ajánlott.
- Most projekt-áttekintés és következő lépések egyeztetése következik.
- Updated docs/examples/example_notes.csv to contain more realistic, user-like notes for a more lifelike demo experience.
- Updated the system to always include raw_text in each entry of the LLM output, updating the schema, prompt, and output formatter accordingly.
- Added logging and output of tool call history: tool invocations are now printed, logged, and included in the LLM output JSON for full traceability.
- Finomítottam a promptot, hogy az agent mindig kérjen pontosítást, ha a tool használat nem egyértelmű, és létrehoztam egy ToolOnlyAgentCore osztályt, amely csak tool használatot engedélyez.
- now 2024-06-13
- Átneveztem a toolokat (clarification_tool -> ask_user, finalize_notes_tool -> finalize_notes), a promptot és a példát is ehhez igazítottam, és készítettem egy demót, ahol az agentnek biztosan az ask_user toolt kell használnia.

